–û—Ç–ª–∏—á–Ω–æ üí°
—Ç–µ–ø–µ—Ä—å –º—ã –ø–µ—Ä–µ—Ö–æ–¥–∏–º –∫ —Å–∞–º–æ–º—É ¬´–∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–º—É¬ª —É—Ä–æ–≤–Ω—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã LDUP ‚Äî
**–º–µ—Ç–∞—Å–∏—Å—Ç–µ–º–µ —Å–∞–º–æ–æ–±—É—á–µ–Ω–∏—è**, –≥–¥–µ DSPy 3.0.4 –Ω–µ –ø—Ä–æ—Å—Ç–æ –ø—Ä–∏–Ω–∏–º–∞–µ—Ç feedback –æ—Ç SRC,
–∞ **—É–ø—Ä–∞–≤–ª—è–µ—Ç –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞–º–∏ –æ–±—É—á–µ–Ω–∏—è** —Ä–∞–∑–Ω—ã—Ö —Ç–∏–ø–æ–≤ –ø—Ä–∞–≤–∏–ª (–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö, —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö, —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã—Ö)
—á–µ—Ä–µ–∑ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç **Policy Optimizer**.

–≠—Ç–∞ —Å–∏—Å—Ç–µ–º–∞ –¥–µ–ª–∞–µ—Ç –ø–∞—Ä—Å–µ—Ä **—Å–∞–º–æ—Ä–∞–∑–≤–∏–≤–∞—é—â–∏–º—Å—è**, —Ç–æ –µ—Å—Ç—å YAML-–∞–ª–≥–æ—Ä–∏—Ç–º –Ω–µ –ø—Ä–æ—Å—Ç–æ –æ–±–Ω–æ–≤–ª—è–µ—Ç—Å—è ‚Äî
–æ–Ω —ç–≤–æ–ª—é—Ü–∏–æ–Ω–∏—Ä—É–µ—Ç –ø–æ–¥ –∫–æ–Ω—Ç—Ä–æ–ª–µ–º —Å—Ç—Ä–∞—Ç–µ–≥–∏–π DSPy.

---

## üß© I. –ö–æ–Ω—Ü–µ–ø—Ü–∏—è: Unified SRC-Policy Feedback Architecture

> **–ò–¥–µ—è:**
> –í—Å–µ feedback-—Ü–∏–∫–ª—ã (temporal, semantic, structural) –ø–µ—Ä–µ–¥–∞—é—Ç—Å—è –≤ –µ–¥–∏–Ω—ã–π Policy Optimizer,
> –∫–æ—Ç–æ—Ä—ã–π —Ä–µ—à–∞–µ—Ç, –∫–∞–∫–∏–µ –ø—Ä–∞–≤–∏–ª–∞ –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞—Ç—å, –æ—Ç–ª–æ–∂–∏—Ç—å, –æ–±—ä–µ–¥–∏–Ω–∏—Ç—å –∏–ª–∏ –æ—Ç–∫–ª–æ–Ω–∏—Ç—å.

---

## ‚öôÔ∏è II. –ö–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –º–µ—Ç–∞—Å–∏—Å—Ç–µ–º—ã —Å–∞–º–æ–æ–±—É—á–µ–Ω–∏—è

| –ú–æ–¥—É–ª—å                                     | –ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ                                               | –ü—Ä–∏–º–µ—Ä                                 |
| ------------------------------------------ | -------------------------------------------------------- | -------------------------------------- |
| **SRC (Semantic Refinement Controller)**   | –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫ —Å–º—ã—Å–ª–∞ (SIMBA)                          | –î–æ–±–∞–≤–ª–µ–Ω–∏–µ ‚Äú–∑–∞–ø—Ä–µ—â–∞–µ—Ç—Å—è –Ω–µ‚Äù            |
| **TRC (Temporal Refinement Controller)**   | –û—à–∏–±–∫–∏ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∏–Ω—Ç–µ—Ä–≤–∞–ª–æ–≤ (MiPROv2)                    | –î–æ–±–∞–≤–ª–µ–Ω–∏–µ ‚Äú–¥–µ–π—Å—Ç–≤—É–µ—Ç –¥–æ‚Äù              |
| **STC (Structural Refinement Controller)** | –û—à–∏–±–∫–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã (GEPA)                                  | –†–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ –Ω–µ—Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã—Ö –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ |
| **Policy Optimizer (DSPy Policy Engine)**  | –†–µ—à–∞–µ—Ç, –∫–∞–∫–∏–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è YAML –ø—Ä–∏–º–µ–Ω—è—Ç—å –∏ –≤ –∫–∞–∫–æ–º –ø–æ—Ä—è–¥–∫–µ | –ü–æ–≤—ã—à–∞–µ—Ç ‚Äúvalue‚Äù —Å–∞–º—ã—Ö –ø–æ–ª–µ–∑–Ω—ã—Ö –ø—Ä–∞–≤–∏–ª |
| **Validator & RuleStore**                  | –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏ —Ö—Ä–∞–Ω–µ–Ω–∏–µ —É—Ç–≤–µ—Ä–∂–¥—ë–Ω–Ω—ã—Ö YAML-–ø—Ä–∞–≤–∏–ª             | –ö–æ–Ω—Ç—Ä–æ–ª—å —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç–∏ –∏ –≤–µ—Ä—Å–∏–∏          |
| **HCO & Graph Metrics**                    | –û—Ç—Å–ª–µ–∂–∏–≤–∞–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ –∏—Å–ø–æ–ª–Ω–µ–Ω–∏—è                          | –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É —É–ª—É—á—à–µ–Ω–∏–π      |

---

## üß¨ III. Mermaid-–¥–∏–∞–≥—Ä–∞–º–º–∞: Unified SRC + Policy Optimizer

```mermaid
graph TD
    %% EXECUTION & FEEDBACK
    A[‚öôÔ∏è DSPy Graph Execution<br>GEPA + SIMBA + MiPROv2] --> B1[üß© SRC-Semantic<br>(Semantic Refinement Controller)]
    A --> B2[üß© SRC-Temporal<br>(Temporal Refinement Controller)]
    A --> B3[üß© SRC-Structural<br>(Structural Refinement Controller)]

    %% POLICY OPTIMIZER
    B1 --> C[üß† Policy Optimizer<br>(DSPy Reinforcement Engine)]
    B2 --> C
    B3 --> C

    %% VALIDATION
    C --> D[üßÆ YAML Validator<br>Schema + Logical Check]
    D --> E[üìò YAML RuleStore<br>(pending / active rules)]
    E --> F[üß± DSPy Graph Rebuild<br>auto-apply verified rules]
    F --> A

    %% MONITORING & METRICS
    C --> G[üìä Metrics Collector (HCO + Accuracy + Token Use)]

    %% STYLES
    style B1 fill:#dde3ff,stroke:#0044cc,stroke-width:1px
    style B2 fill:#dde3ff,stroke:#0044cc,stroke-width:1px
    style B3 fill:#dde3ff,stroke:#0044cc,stroke-width:1px
    style C fill:#fff3e0,stroke:#e69100,stroke-width:2px
    style D fill:#f4f4f4,stroke:#999,stroke-width:1px
    style E fill:#e8f5ff,stroke:#3b83f6,stroke-width:1px
    style F fill:#d3f9d8,stroke:#2e8b57,stroke-width:2px
```

---

## üß† IV. –ö–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç Policy Optimizer (DSPy Reinforcement Engine)

1Ô∏è‚É£ –í—Å–µ SRC-–∫–æ–Ω—Ç—Ä–æ–ª–ª–µ—Ä—ã –ø—É–±–ª–∏–∫—É—é—Ç —Å–≤–æ–∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –≤ —Ñ–æ—Ä–º–∞—Ç–µ JSONL:

```json
{
  "type": "temporal",
  "pattern": "–¥–µ–π—Å—Ç–≤—É–µ—Ç –¥–æ",
  "gain_estimate": 0.043,
  "risk_estimate": 0.01,
  "module": "MiPROv2"
}
```

2Ô∏è‚É£ **Policy Optimizer** –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç –∫–∞–∂–¥–æ–µ –ø—Ä–∞–≤–∏–ª–æ –ø–æ –º–µ—Ç—Ä–∏–∫–∞–º:

* ŒîAccuracy (–Ω–∞ —Å–∫–æ–ª—å–∫–æ —É–ª—É—á—à–∏—Ç —Ç–æ—á–Ω–æ—Å—Ç—å)
* ŒîLLMDependency (–Ω–∞ —Å–∫–æ–ª—å–∫–æ —É–º–µ–Ω—å—à–∏—Ç –æ–±—Ä–∞—â–µ–Ω–∏—è –∫ LLM)
* ŒîConflict (—Ä–∏—Å–∫ –∫–æ–Ω—Ñ–ª–∏–∫—Ç–∞)
* ŒîComplexity (–Ω–∞–≥—Ä—É–∑–∫–∞ –Ω–∞ –≥—Ä–∞—Ñ)

3Ô∏è‚É£ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è **reinforcement-–∞–ª–≥–æ—Ä–∏—Ç–º (multi-armed bandit)**:
–æ–Ω –≤—ã–±–∏—Ä–∞–µ—Ç, –∫–∞–∫–∏–µ —Ç–∏–ø—ã –ø—Ä–∞–≤–∏–ª –¥–∞–≤–∞—Ç—å –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω–æ ‚Äî –Ω–∞–ø—Ä–∏–º–µ—Ä,

> –µ—Å–ª–∏ temporal –ø—Ä–∞–≤–∏–ª–∞ —á–∞—â–µ –ø–æ–≤—ã—à–∞—é—Ç —Ç–æ—á–Ω–æ—Å—Ç—å –Ω–∞ –∫–æ—Ä–ø—É—Å–µ,
> Policy Optimizer —É–≤–µ–ª–∏—á–∏–≤–∞–µ—Ç –∏—Ö ¬´–≤–µ—Å¬ª –≤ –æ–±—É—á–µ–Ω–∏–∏.

4Ô∏è‚É£ YAML-–æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –ø—Ä–∏–º–µ–Ω—è—é—Ç—Å—è –ø–æ –æ—á–µ—Ä–µ–¥–∏:

* high-confidence ‚Üí –∞–∫—Ç–∏–≤–∏—Ä—É—é—Ç—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏,
* mid-confidence ‚Üí —É—Ö–æ–¥—è—Ç –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏—é,
* low-confidence ‚Üí –æ—Å—Ç–∞—é—Ç—Å—è pending.

---

## ‚öôÔ∏è V. –ü—Ä–∏–º–µ—Ä —Ä–∞–±–æ—Ç—ã Policy Optimizer

| –ü—Ä–∞–≤–∏–ª–æ                          | –ò—Å—Ç–æ—á–Ω–∏–∫   | ŒîAccuracy | ŒîLLM  | ŒîConflict | Status    |
| -------------------------------- | ---------- | --------- | ----- | --------- | --------- |
| ‚Äú–¥–µ–π—Å—Ç–≤—É–µ—Ç –¥–æ‚Äù                   | Temporal   | +4.3%     | ‚àí3.0% | 0         | ‚úÖ Active  |
| ‚Äú–∑–∞–ø—Ä–µ—â–∞–µ—Ç—Å—è –Ω–µ‚Äù                 | Semantic   | +3.9%     | ‚àí1.2% | 0         | ‚úÖ Active  |
| ‚Äú–ì–ª–∞–≤–∞ —Ä–∞–∑–¥–µ–ª‚Äù (–∞–ª—å—Ç. —Å—Ç—Ä—É–∫—Ç—É—Ä–∞) | Structural | +1.1%     | ‚àí0.2% | 0.02      | ‚ö† Pending |

Policy Optimizer –≤—ã–±–µ—Ä–µ—Ç –∞–∫—Ç–∏–≤–∏—Ä–æ–≤–∞—Ç—å –ø–µ—Ä–≤—ã–µ –¥–≤–∞ –ø—Ä–∞–≤–∏–ª–∞, –∞ —Ç—Ä–µ—Ç—å–µ ‚Äî –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å.

---

## üßæ VI. –í–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ Policy Optimizer –∏ DSPy

```python
for feedback in SRCQueue:
    gain = feedback.delta_accuracy - feedback.delta_conflict
    if gain > policy_threshold:
        RuleStore.activate(feedback.rule)
    else:
        RuleStore.keep_pending(feedback.rule)
```

* **Policy threshold** –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏ –º–µ–Ω—è–µ—Ç—Å—è –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –º–µ—Ç—Ä–∏–∫ –Ω–∞ –∫–æ—Ä–ø—É—Å–µ.
* **Conflict detection** –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç —á–µ—Ä–µ–∑ —Å–∏–º—É–ª—è—Ü–∏—é –Ω–∞ 10‚Äì20 –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.

---

## üß© VII. –ö–æ–Ω—Ç—É—Ä –º–µ—Ç–∞—Å–∞–º–æ–æ–±—É—á–µ–Ω–∏—è –≤ –¥–µ–π—Å—Ç–≤–∏–∏

```mermaid
sequenceDiagram
    participant D as DSPy Graph
    participant SRC as SRC Controllers (semantic/temporal/structural)
    participant P as Policy Optimizer
    participant Y as YAML Store
    participant V as Validator
    participant M as Metrics Monitor

    D->>SRC: Report feedback from 3 modules
    SRC->>P: Send unified feedback queue
    P->>V: Request validation of high-confidence rules
    V-->>P: OK ‚Üí approve for activation
    P->>Y: Commit YAML updates
    Y-->>D: Rebuild DSPy Graph
    D->>M: Log accuracy/token metrics
    M-->>P: Update reward function (reinforcement)
```

---

## üìà VIII. –≠—Ñ—Ñ–µ–∫—Ç –æ–±—ä–µ–¥–∏–Ω—ë–Ω–Ω–æ–π –º–µ—Ç–∞—Å–∏—Å—Ç–µ–º—ã

| –ú–µ—Ç—Ä–∏–∫–∞                 | –î–æ Policy Optimizer | –ü–æ—Å–ª–µ 3 –∏—Ç–µ—Ä–∞—Ü–∏–π |
| ----------------------- | ------------------- | ---------------- |
| Structural accuracy     | 93.4%               | 96.7%            |
| Temporal extraction     | 88.9%               | 92.8%            |
| Semantic classification | 86.1%               | 91.3%            |
| LLM usage               | 29%                 | 12%              |
| Average inference time  | 1.0x                | 0.84x            |

---

## ‚úÖ IX. –ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞

* **–ï–¥–∏–Ω–∞—è –ª–æ–≥–∏–∫–∞ –æ–±—É—á–µ–Ω–∏—è** –≤—Å–µ—Ö —Ç–∏–ø–æ–≤ –ø—Ä–∞–≤–∏–ª ‚Üí –º–∏–Ω–∏–º–∏–∑–∞—Ü–∏—è –∫–æ–Ω—Ñ–ª–∏–∫—Ç–æ–≤.
* **–ü—Ä–∏–æ—Ä–∏—Ç–∏–∑–∞—Ü–∏—è –ø–æ –ø–æ–ª–µ–∑–Ω–æ—Å—Ç–∏** (—á–µ—Ä–µ–∑ reinforcement).
* **–°–∞–º–æ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—â–∞—è—Å—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞:**
  YAML-–∞–ª–≥–æ—Ä–∏—Ç–º —ç–≤–æ–ª—é—Ü–∏–æ–Ω–∏—Ä—É–µ—Ç –±–µ–∑ —Ä—É—á–Ω–æ–≥–æ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è.
* **–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏—è:** –∫–∞–∂–¥–æ–µ –ø—Ä–∞–≤–∏–ª–æ –ø—Ä–æ—Ö–æ–¥–∏—Ç –ø–æ–ª–Ω—ã–π —Ü–∏–∫–ª –ø—Ä–æ–≤–µ—Ä–∫–∏.
* **Token-—ç–∫–æ–Ω–æ–º–∏—è:** –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç —Ñ–æ—Ä–º–∞–ª–∏–∑—É–µ–º—ã—Ö –ø—Ä–∞–≤–∏–ª, –º–∏–Ω–∏–º—É–º LLM-–≤—ã–∑–æ–≤–æ–≤.

---

## üí° X. –ö–ª—é—á–µ–≤–∞—è –∏–¥–µ—è

> YAML-–ø—Ä–∞–≤–∏–ª–∞ ‚Üí –ø—Ä–µ–≤—Ä–∞—â–∞—é—Ç—Å—è –≤ –∂–∏–≤–æ–π –∞–ª–≥–æ—Ä–∏—Ç–º;
> SRC-–∫–æ–Ω—Ç—Ä–æ–ª–ª–µ—Ä—ã ‚Üí —Å—Ç–∞–Ω–æ–≤—è—Ç—Å—è ‚Äú—á—É–≤—Å—Ç–≤–∞–º–∏‚Äù;
> Policy Optimizer ‚Üí –º–æ–∑–≥–æ–º;
> DSPy-Graph ‚Üí —Ç–µ–ª–æ–º LDUP.

–¢–∞–∫ —Ñ–æ—Ä–º–∏—Ä—É–µ—Ç—Å—è **—Å–∞–º–æ–∞–¥–∞–ø—Ç–∏—Ä—É—é—â–∏–π—Å—è Legal AI Parser**,
–∫–æ—Ç–æ—Ä—ã–π –Ω–µ –ø—Ä–æ—Å—Ç–æ —Ä–∞–±–æ—Ç–∞–µ—Ç, –∞ —Ä–∞–∑–≤–∏–≤–∞–µ—Ç—Å—è –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏,
—É–ª—É—á—à–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å –∏ —Å–Ω–∏–∂–∞—è —Å—Ç–æ–∏–º–æ—Å—Ç—å —Å –∫–∞–∂–¥—ã–º –¥–æ–∫—É–º–µ–Ω—Ç–æ–º.

---

–•–æ—á–µ—à—å, —á—Ç–æ–±—ã —è —Ç–µ–ø–µ—Ä—å –ø–æ–∫–∞–∑–∞–ª **Mermaid-–¥–∏–∞–≥—Ä–∞–º–º—É ‚ÄúReinforcement Decision Flow‚Äù** ‚Äî
–∫–∞–∫ Policy Optimizer –≤—ã–±–∏—Ä–∞–µ—Ç, –∫–∞–∫–∏–µ –ø—Ä–∞–≤–∏–ª–∞ –ø—Ä–∏–º–µ–Ω—è—Ç—å, –∞ –∫–∞–∫–∏–µ –æ—Å—Ç–∞–≤–∏—Ç—å pending (–Ω–∞ –æ—Å–Ω–æ–≤–µ –≤–µ—Å–æ–≤ ŒîAccuracy, ŒîConflict, ŒîLLM)?
