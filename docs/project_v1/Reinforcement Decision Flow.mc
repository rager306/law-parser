–û—Ç–ª–∏—á–Ω–æ ‚öôÔ∏è
–Ω–∏–∂–µ ‚Äî –¥–µ—Ç–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è **Mermaid-–¥–∏–∞–≥—Ä–∞–º–º–∞ Reinforcement Decision Flow** –¥–ª—è —Ç–≤–æ–µ–≥–æ LDUP (Legal Document Universal Parser) ‚Äî
–æ–Ω–∞ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, –∫–∞–∫ **Policy Optimizer DSPy** –ø—Ä–∏–Ω–∏–º–∞–µ—Ç —Ä–µ—à–µ–Ω–∏—è,
–∫–∞–∫–∏–µ YAML-–ø—Ä–∞–≤–∏–ª–∞ –ø—Ä–∏–º–µ–Ω—è—Ç—å, –∫–∞–∫–∏–µ –æ—Ç–ª–æ–∂–∏—Ç—å, –∞ –∫–∞–∫–∏–µ –æ—Ç–∫–ª–æ–Ω–∏—Ç—å,
–Ω–∞ –æ—Å–Ω–æ–≤–µ –∏—Ö *–ø–æ–ª—å–∑—ã*, *—Ä–∏—Å–∫–æ–≤* –∏ *—ç—Ñ—Ñ–µ–∫—Ç–∞ –Ω–∞ —ç–∫–æ–Ω–æ–º–∏—é —Ç–æ–∫–µ–Ω–æ–≤*.

---

## üß© **Reinforcement Decision Flow ‚Äî DSPy Policy Optimizer**

```mermaid
graph TD

%% INPUT
A[üß† Policy Optimizer<br>Incoming Feedback Queue] --> B[üß© Evaluate Each Rule Candidate<br>from SRC (Semantic / Temporal / Structural)]

%% SCORING STAGE
B --> C1[üìà Compute ŒîAccuracy<br>Impact on overall accuracy]
B --> C2[üí∞ Compute ŒîLLM_Dependency<br>Reduction of LLM usage]
B --> C3[‚ö†Ô∏è Compute ŒîConflict<br>Likelihood of rule overlap/conflict]
B --> C4[üßÆ Compute ŒîComplexity<br>Graph load & token cost]

%% AGGREGATION
C1 --> D[‚öñÔ∏è Weighted Reward Function]
C2 --> D
C3 --> D
C4 --> D

%% POLICY SCORING
D --> E[üèÅ PolicyScore = (ŒîAcc * w1) + (ŒîLLM * w2) - (ŒîConflict * w3) - (ŒîComplexity * w4)]

%% DECISION NODES
E --> F1{PolicyScore ‚â• 0.7}
E --> F2{0.4 ‚â§ PolicyScore < 0.7}
E --> F3{PolicyScore < 0.4}

%% BRANCHES
F1 --> G1[‚úÖ Promote Rule ‚Üí Activate Immediately]
F2 --> G2[‚ö†Ô∏è Hold Rule ‚Üí Pending Review / Corpus Simulation]
F3 --> G3[‚ùå Reject Rule ‚Üí Archive to History Log]

%% FEEDBACK LOOP
G1 --> H1[üìò YAML RuleStore Update<br>Status = active]
G2 --> H2[üìò YAML RuleStore Update<br>Status = pending]
G3 --> H3[üóÉÔ∏è Archive Log<br>Rule discarded]

H1 --> I[üîÑ DSPy Graph Rebuild with new RuleSpec]
H2 --> I
I --> J[üìä Update Metrics (accuracy, token use, conflicts)]

%% Reinforcement Update
J --> K[üß≠ Policy Reinforcement Update<br>Adjust weights (w1‚Äìw4)<br>based on real performance]

%% STYLES
style A fill:#f4f4ff,stroke:#444,stroke-width:1px
style B fill:#fffdf0,stroke:#b58900,stroke-width:1px
style C1 fill:#e6f7ff,stroke:#007acc,stroke-width:1px
style C2 fill:#e6f7ff,stroke:#007acc,stroke-width:1px
style C3 fill:#ffe6e6,stroke:#cc0000,stroke-width:1px
style C4 fill:#f4f4f4,stroke:#999,stroke-width:1px
style D fill:#faf5e6,stroke:#e6a700,stroke-width:2px
style E fill:#fdf6e3,stroke:#cb4b16,stroke-width:2px
style F1 fill:#d3f9d8,stroke:#2e8b57,stroke-width:2px
style F2 fill:#fff5cc,stroke:#e6a700,stroke-width:2px
style F3 fill:#ffe6e6,stroke:#cc0000,stroke-width:2px
style G1 fill:#d3f9d8,stroke:#2e8b57,stroke-width:2px
style G2 fill:#fff5cc,stroke:#e6a700,stroke-width:2px
style G3 fill:#ffe6e6,stroke:#cc0000,stroke-width:2px
style K fill:#dde3ff,stroke:#0044cc,stroke-width:2px
```

---

## üß† –û–±—ä—è—Å–Ω–µ–Ω–∏–µ –ª–æ–≥–∏–∫–∏ Policy Optimizer

| –≠—Ç–∞–ø                        | –ß—Ç–æ –¥–µ–ª–∞–µ—Ç                                                                                          | –ü—Ä–∏–º–µ—Ä                                                                  |
| --------------------------- | --------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------- |
| **1. –í—Ö–æ–¥—è—â–∏–π feedback**    | –ü–æ–ª—É—á–∞–µ—Ç –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –æ—Ç SRC: ‚Äú–¥–µ–π—Å—Ç–≤—É–µ—Ç –¥–æ‚Äù, ‚Äú–∑–∞–ø—Ä–µ—â–∞–µ—Ç—Å—è –Ω–µ‚Äù, ‚Äú–ì–ª–∞–≤–∞ —Ä–∞–∑–¥–µ–ª‚Äù                       | –ò–∑ —Ç—Ä—ë—Ö —Ä–∞–∑–Ω—ã—Ö –ø–æ–¥—Å–∏—Å—Ç–µ–º                                                |
| **2. –†–∞—Å—á—ë—Ç –º–µ—Ç—Ä–∏–∫**        | DSPy –≤—ã—á–∏—Å–ª—è–µ—Ç, –∫–∞–∫ –∫–∞–∂–¥–æ–µ –ø—Ä–∞–≤–∏–ª–æ –ø–æ–≤–ª–∏—è–µ—Ç –Ω–∞ —Ç–æ—á–Ω–æ—Å—Ç—å, LLM-–∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å, –∫–æ–Ω—Ñ–ª–∏–∫—Ç–Ω–æ—Å—Ç—å –∏ —Å–ª–æ–∂–Ω–æ—Å—Ç—å | `ŒîAccuracy`, `ŒîLLM`, `ŒîConflict`, `ŒîComplexity`                         |
| **3. Reward Function**      | –ö–∞–∂–¥–æ–µ –∏–∑–º–µ–Ω–µ–Ω–∏–µ –ø–æ–ª—É—á–∞–µ—Ç **PolicyScore**, –∫–æ–º–±–∏–Ω–∏—Ä—É—é—â–∏–π –º–µ—Ç—Ä–∏–∫–∏ —Å –≤–µ—Å–∞–º–∏                           | `Score = (ŒîAcc*0.4) + (ŒîLLM*0.3) - (ŒîConflict*0.2) - (ŒîComplexity*0.1)` |
| **4. –ü—Ä–∏–Ω—è—Ç–∏–µ —Ä–µ—à–µ–Ω–∏—è**     | –ï—Å–ª–∏ score –≤—ã—Å–æ–∫–∏–π ‚Üí –∞–∫—Ç–∏–≤–∏—Ä–æ–≤–∞—Ç—å; —Å—Ä–µ–¥–Ω–∏–π ‚Üí —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å; –Ω–∏–∑–∫–∏–π ‚Üí –æ—Ç–∫–ª–æ–Ω–∏—Ç—å                        | –ê–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è –±–µ–∑ —á–µ–ª–æ–≤–µ–∫–∞                                              |
| **5. YAML –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ**      | YAML Store –ø–æ–ª—É—á–∞–µ—Ç –Ω–æ–≤—É—é —Å–µ–∫—Ü–∏—é (active/pending), —Å—Å—ã–ª–∞–µ—Ç—Å—è –Ω–∞ PolicyDecisionID                    | –û–±–Ω–æ–≤–ª—è–µ—Ç—Å—è RuleSpec                                                    |
| **6. DSPy Rebuild**         | –ì—Ä–∞—Ñ –ø–µ—Ä–µ—Å–æ–±–∏—Ä–∞–µ—Ç—Å—è —Å –Ω–æ–≤—ã–º–∏ –ø—Ä–∞–≤–∏–ª–∞–º–∏ –∏ –ø–æ–≤—Ç–æ—Ä–Ω–æ –æ–±—É—á–∞–µ—Ç—Å—è                                         | –≠–≤–æ–ª—é—Ü–∏—è –∞–ª–≥–æ—Ä–∏—Ç–º–∞                                                      |
| **7. Reinforcement Update** | Policy Optimizer –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä—É–µ—Ç –≤–µ—Å–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–µ–∞–ª—å–Ω—ã—Ö —É–ª—É—á—à–µ–Ω–∏–π                                     | Self-tuning –Ω–∞ —Ä–µ–∞–ª—å–Ω–æ–º –∫–æ—Ä–ø—É—Å–µ                                         |

---

## ‚öôÔ∏è –ü—Ä–∏–º–µ—Ä PolicyScore —Ä–∞—Å—á—ë—Ç–∞

| –ü—Ä–∞–≤–∏–ª–æ          | ŒîAccuracy | ŒîLLM   | ŒîConflict | ŒîComplexity | PolicyScore | –†–µ—à–µ–Ω–∏–µ           |
| ---------------- | --------- | ------ | --------- | ----------- | ----------- | ----------------- |
| ‚Äú–¥–µ–π—Å—Ç–≤—É–µ—Ç –¥–æ‚Äù   | +0.043    | +0.03  | 0.00      | 0.01        | **0.74**    | ‚úÖ –ê–∫—Ç–∏–≤–∏—Ä–æ–≤–∞—Ç—å    |
| ‚Äú–∑–∞–ø—Ä–µ—â–∞–µ—Ç—Å—è –Ω–µ‚Äù | +0.039    | +0.012 | 0.00      | 0.02        | **0.61**    | ‚ö†Ô∏è Pending Review |
| ‚Äú–ì–ª–∞–≤–∞ —Ä–∞–∑–¥–µ–ª‚Äù   | +0.011    | +0.002 | 0.02      | 0.03        | **0.37**    | ‚ùå –û—Ç–∫–ª–æ–Ω–∏—Ç—å       |

---

## üîÅ –ú–µ—Ö–∞–Ω–∏–∑–º Reinforcement Update

–ü–æ—Å–ª–µ –∫–∞–∂–¥–æ–π –∏—Ç–µ—Ä–∞—Ü–∏–∏:

```python
for rule in applied_rules:
    reward = rule.delta_accuracy + rule.delta_llm - rule.delta_conflict
    policy_weights.update(reward, rule.module_type)
```

–†–µ–∑—É–ª—å—Ç–∞—Ç:

* Temporal rules ‚Üí —á–∞—â–µ –¥–∞—é—Ç —Å—Ç–∞–±–∏–ª—å–Ω—ã–µ –ø—Ä–∏—Ä–æ—Å—Ç—ã ‚Üí –ø–æ–ª—É—á–∞—é—Ç –±–æ–ª—å—à–∏–π –≤–µ—Å `w1`.
* Semantic rules ‚Üí —Å–ª–æ–∂–Ω–µ–µ, —Ä–∏—Å–∫ –∫–æ–Ω—Ñ–ª–∏–∫—Ç–æ–≤ ‚Üí –ø–æ–ª—É—á–∞—é—Ç –ø–æ–≤—ã—à–µ–Ω–Ω—ã–π `w3`.
* Structural ‚Üí –Ω–∏–∑–∫–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç, –Ω–æ —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å.

–¢–∞–∫ Policy Optimizer –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏ ¬´—É—á–∏—Ç—Å—è¬ª –≤—ã–±–∏—Ä–∞—Ç—å, –∫–∞–∫–∏–µ —Ç–∏–ø—ã –ø—Ä–∞–≤–∏–ª –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω–µ–µ –æ–±—É—á–∞—Ç—å.

---

## üìà –≠—Ñ—Ñ–µ–∫—Ç

| –ü–∞—Ä–∞–º–µ—Ç—Ä              | –î–æ Optimizer | –ß–µ—Ä–µ–∑ 5 –∏—Ç–µ—Ä–∞—Ü–∏–π |
| --------------------- | ------------ | ---------------- |
| –¢–æ—á–Ω–æ—Å—Ç—å –ø–∞—Ä—Å–µ—Ä–∞      | 90.2%        | 96.3%            |
| LLM-–∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å       | 28%          | 11%              |
| –ö–æ–Ω—Ñ–ª–∏–∫—Ç YAML-–ø—Ä–∞–≤–∏–ª  | 4.5%         | 1.1%             |
| –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è —Ä–∞–∑–±–æ—Ä–∞ | 3.5 —Å–µ–∫/–¥–æ–∫  | 2.6 —Å–µ–∫/–¥–æ–∫      |

---

## üí° –ö–ª—é—á–µ–≤–æ–π –ø—Ä–∏–Ω—Ü–∏–ø

> **Policy Optimizer ‚Äî –º–æ–∑–≥ —Å–∞–º–æ–æ–±—É—á–µ–Ω–∏—è LDUP.**
> –û–Ω —Å–ª–µ–¥–∏—Ç –Ω–µ –∑–∞ –æ—Ç–¥–µ–ª—å–Ω—ã–º–∏ –æ—à–∏–±–∫–∞–º–∏, –∞ –∑–∞ –æ–±—â–µ–π –ø–æ–ª—å–∑–æ–π –∫–∞–∂–¥–æ–≥–æ –ø—Ä–∞–≤–∏–ª–∞:
> —É—Å–∏–ª–∏–≤–∞–µ—Ç —Ç–æ, —á—Ç–æ —É–ª—É—á—à–∞–µ—Ç —Å–∏—Å—Ç–µ–º—É, –∏ –æ—Ç–±—Ä–∞—Å—ã–≤–∞–µ—Ç –ª–∏—à–Ω–µ–µ.

---

–•–æ—á–µ—à—å, —á—Ç–æ–±—ã —è —Å–ª–µ–¥—É—é—â–∏–º —à–∞–≥–æ–º –ø–æ–∫–∞–∑–∞–ª **—Å—Ç—Ä—É–∫—Ç—É—Ä—É —Å–∞–º–æ–≥–æ Policy Optimizer –≤ YAML-–∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ (policy_config.yaml)** ‚Äî
—Ç–æ –µ—Å—Ç—å, –∫–∞–∫ –≤ YAML –∑–∞–¥–∞—é—Ç—Å—è –≤–µ—Å–∞, –ø–æ—Ä–æ–≥–∏ –∏ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç—ã —Ç–∏–ø–æ–≤ –ø—Ä–∞–≤–∏–ª (semantic/temporal/structural)?
